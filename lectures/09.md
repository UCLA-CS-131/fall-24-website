---
title: 09. Data Palooza 3
week: 5
lecture_date: 2024-10-28
author: Einar Balan
originally_written: 2022-10-24
original_author: Matt Wang, Siddarth Krishnamoorthy, and Einar Balan
layout: lecture
parent: Lecture Notes
---

{: .note }
This lecture note covers slides 79-141 of Data Palooza.

## Table of Contents
{: .no_toc }

{:toc}
- dummy item

## Scoping

### Scope and "In Scope"

The **scope** of a variable is the part of a program where a variable is valid (i.e., can be accessed). The parts can be lines, statements, expressions, instructions, or other units!

A variable is **"in scope"** in a part of a program if it is *currently* accessible by name.

For this example,

```cpp
void foo() {
  int x;
  cout << x;
}
```

we'd say:

- "The scope of the `x` variable is the function `foo()`."
- "The x variable is in scope within the `foo` function because it is defined at the top of the function."

**Scoping strategies** describe when variables are in-scope!

Here's a simple example from C++:

```cpp
void foo() {
  int x;
  cout << x; // Just fine, x is in foo’s scope!
}

void bar() {
  cout << x; // ERROR! x isn’t in bar’s scope!
}
```

C++ uses a lexical scoping strategy that scopes to blocks (we'll explain this soon).

### Lexical Environments

Here's a slightly more in-depth example:

```cpp
string dinner = "burgers";     // this variable's scope is global - it's in scope everywhere

void party(int drinks) {       // drinks is in scope for the entire party() function
  cout << "Partay! w00t";
  if (drinks > 2) {
    bool puke = true;          // bool is only in scope for this if statement
    cout << "Puked " << dinner;
  }
}

void study(int hrs) {          // hrs is in scope for the entire study() function; but, not the same hrs as main()!
  int drinks = 2;              // drinks is also in scope for study(); but, not the same drinks as party()!
  cout << "Study for " << hrs;
  party(drinks+1);
}

int main() {
  int hrs = 10;                // this hrs is only in scope in main(); different from the hrs in study()!
  study(hrs-1);
}
```

Functions have their own scope too: they are defined immediately after they're declared.

As the program runs, it maintains a **lexical environment**: a tracker of what's in scope and what's not!

- for example, in the `main()` function, the lexical environment only contains the variable `hrs` and the three functions `main()`, `study()`, and `party()`
- but, once we call `study(hrs-1)` and enter a new environment, the lexical environment changes: we get a different `hrs` (the one in the argument), as well as a new variable `drinks`
- for a full trace, play through the animations on slide 83!

Many languages have *slightly different* scoping strategies! Differences include:

- how do constructs like functions, control flow (if statements, loops), or classes affect varaible scope?
- what do you do if you try to define a new variable with a name that already exists?
- when does a variable's scope end?

By the end of this section, we'll know how to answer these questions for all sorts of functions :)

### Lifetime

There is one related but different concept: a lifetime.

A variable's **lifetime** describes when a variable can be accessed. It includes times when the variable is in scope, but also times where it's not in scope (but can be accessed indirectly).

Values also have lifetimes. Variables and the values they point to can have different lifetimes!

Take a look at our previous example, but slightly modified:

```cpp
void study(int how_long) {
  while (how_long-- > 0)
    cout << "Study!\n";
  cout << "Partay!\n";
}

int main() {
  int hrs = 10;
  study(hrs);
  cout << "I studied " << hrs <<
        " hours!";
}
```

The scope of `hrs` is only within `main()`; when we call into `study`, we can no longer access it. But, **its lifetime also extends to `study()`**, since we can access it indirectly - through the variable `how_long`!

Python lets you manually end a variable's lifetime early:

```py
def main():
  var = "I exist"
    ...
  del var    # no longer exists!
  print(var) # error!
```

### Lexical Scoping

Lexical scoping is the most-used scoping paradigm (almost all languages you've used use it). The core tenet has to do with how your code is organized, which we'll call the **context** that a variable is defined in. If we can't find the variable in the current context, we'll go to the enclosing context - until we run out (and hit the global context).

More strictly, languages like Python use the **LEGB** rule. We look for a variable in this order:

1. First, look at the **local scope**. In Python, this is either an expression or a function.
2. If it's not there, look at successive **enclosing scopes**. Repeat this until...
3. You hit the **global scope**.
4. If it's still not there, look at the **built-in scope** - this contains things like `print`

Importantly, once a context is over, the variables in that context can't be accessed!

To get a feel of lexical scoping, let's go through a few examples.

In Python, one context type is an **expression**:

```py
sum([x*x for x in range(10)])     # this works :)
sum([x*x for x in range(10)] + x) # x isn't in scope!
```

In this example, `x` is defined in the context of an **expression** (a list comprehension). Its scope is only in that expression!

In C++, one context type is a **block** (roughly: any set of braces, `{` and `}`):

```cpp
if (drinks > 2) {
  int puke = 5;
  // ...
}
cout << puke; // error - puke is not in scope!
```

Note that Python does not scope with respect to blocks!

```py
def a():
  if True:
    x = 5
  print(x) # this works!
```

However, both Python and C++ scope to **functions**:

```py
def a():
  x = 5

print(x) # error - x is out of scope!
```

```cpp
void snore(int n) {
  int i = 0;
  while (i++ < n){ /* */}
}
cout << i; // error - i is not in scope!
```

Other relevant scoping constructs include:

- classes: think about private functions and member variables!
- namespaces: think about `using namespace std;` versus having to type out `std::cout`
- the **global** scope: the last resort!


### Shadowing

When a variable is redefined in a different context, lexical scoping languages typically use an approach called **shadowing**. In an inner context, the redefined variable "replaces" all uses of the outer context variable; once the inner context is finished, we return to the outer context variable.

Here's a classic example from the [Wikipedia page on shadowing](https://en.wikipedia.org/wiki/Variable_shadowing) for Python:

```py
def outer():
  x = 1
  def inner():
    x = 2
    print("inner:", x)
  inner()
  print("outer:", x)
outer()
print("global:", x)
```

and in C++:

```cpp
int main(){
  int x = 42;
  int sum = 0;

  for (int i = 0; i < 10; i++) {
    int x = i;
    std::cout << "x: " << x << '\n'; // prints values of i from 0 to 9
    sum += x;
  }

  std::cout << "sum: " << sum << '\n';
  std::cout << "x:   " << x   << '\n'; // prints out 42
}
```

Shadowing is not *great* practice (though there are legitimate uses!). However, you'll encounter it frequently when reading code, and *accidentally*! For example, consider this function in Python:

```py
def a(input):  # this shadows the global input!
  print(input)
  a = input()  # this doesn't work anymore!!
```

So, you have to be careful with shadowing!!

### Dynamic Scoping

**Dynamic scoping** takes a different approach: the value of a variable is always the most recently defined (or redefined) version, regardless of the lexical scope! For dynamic scoping, what matters is the **chronological order** variables are defined in: not how the code is structured.

Here's a quick example:

```
func foo() {
  x = x + 1;
  print x
}

func bar() {
  x = 10;
  foo();
}

func main() {
  x = 1;
  foo();
  bar();
}

// prints:
// 2
// 11
```

The interesting note is that `foo()` or `bar()` didn't need arguments or parameters. After defining `x` once, its value "persists" across `foo()`, `bar()`, and the subsequent call to `foo()`! Neat!

These days, few languages use it. Here's one example from Emacs Lisp:

```lisp
(setq a 100)  ; sets a to 100

; prints the value of a
(defun print_value_of_a ()
  (print a))

; define local variable a, then
; call print_value_of_a
(let ((a -42))
  (print_value_of_a))

; this prints: -42
```


## Memory Safety (and Management)

Memory safety is a key property of strongly-typed languages. In short, memory safe languages prevent any memory operations that could lead to undefined behaviour, while memory unsafe languages allow such operations. These include:

- out-of-bound array indexes and unconstrained pointer arithmetic
- casting values to incompatible types
- use of uninitialized variables and pointers
- use of dangling pointers to dead objects (use-after-free, double-free, ...)

These are the *biggest* source of security vulnerabilities in low-level code. In memory safe languages, all the above operations are usually not allowed at compile-time (ex pointer arithmetic or invalid casts) or runtime errors (out-of-bound array indexes).

Do memory leaks lead to undefined behaviour? No! Since memory leaks only lead to unused memory not being freed, they don't lead to undefined behaviour. Even if the system runs out of memory due to the memory leak, the program is *predictably* terminated. Therefore, we see no undefined behaviour.

A large innovation in programming languages over the past 50 years is in *garbage collection*, a form of memory management. Among other things, it helps ensure memory safety in a program; so, we'll devote a huge chunk of content on it! We'll also contrast this with other approaches like the ownership model (of Rust fame), manual memory management, etc.

## Garbage Collection

{: .note }
If you're interested in software engineering, a *ton* of great work goes into optimizing GCs and memory management. And, it lets us talk about Rust!

Garbage collection is the automatic reclamation of memory which was allocated by a program, but is no longer referenced in code. In languages with garbage collection (like Python), the programmer *does not* need to explicitly control object destruction - the languages automatically handles that for the programmer. When a value or an object on the heap is no longer referrenced, the program (eventually) detects this at run time and frees the memory associated with it.

Garbage collection has multiple advantages:
- It eliminates memory leaks. This ensures memory allocated for objects is freed once it's no longer needed
- It eliminates dangling pointers and the use of dead objects. This prevents access to objects after they have been de-allocated
- It eliminates double-free bugs. This eliminates inadvertent attempts to free memory more than once
- It eliminates manual memory management. This simplifies code by eliminating manual deletion of memory

However, reasoning about garbage collection is tricky - it's one of our first language behaviours that's "nondeterministic".

### Approaches to garbage collection

A good rule of thumb is that we should only garbage collect an object when there are no longer any references to that object. For example, if an object goes out of scope, it should be destroyed.

```java
public void do_some_work() {
    Nerd nerd = new Nerd("Jen");
    ...
} // nerd goes out of scope

```
Another example, if an object reference (pointer) is overriden, then the old value should be deleted (as long as it's not being used anywhere else).
```java
public void do_some_work() {
    Nerd nerd = new Nerd("Jen");
    ...
    // we overwrite an obj ref
    nerd = new Nerd("Rick");
    // or
    nerd = null;
}
```

We'll focus on three (of many) major approaches to GC:

- mark and sweep
- mark and compact
- reference counting

The first two are part of a class of GC algorithms called "tracing algorithms", and operate in "bulk" - they do it all at once. The latter instead uses a different metric.

### Mark and Sweep

Mark and Sweep runs in two phases: a **mark** phase and a **sweep** phase. In the mark phase, the program identifies all objects that are still referred to and thus considered to be in-use. In the sweep phase, the algorithm scans all heap memory from start to finish, and frees all blocks not marked as being 'in-use.'

#### Mark

During the mark phase, our goal is to discover all active objects that are still being used. We consider an object in-use (and its memory not reclaimable) if it meets one of two criteria:

- it is one of a key set of *root* objects. Root objects include global variables, local variables across all stack frames, and parameters on the call stack.
- it is reachable from a root object. If an object can be transitively reached via one or more pointers/references from a root object (e.g.,robot object points to battery).

At a high-level, the GC identifies all root objects and adds their object references to a queue. Then, the GC uses the queue to breadth-first search from the root objects and mark all the reachable objects as "in-use". To do this, each object is augmented with a bit which is set by the GC to mark that it is in use. Once all reachable objects have been marked, all unmarked objects can be disposed of.

Here's some pseudocode for a possible mark algorithm:

```python
# Pseudocode for the Mark algorithm
def mark():
  roots = get_all_root_objs()
  candidates = new Queue()
  for each obj_ref in roots:
    candidates.enqueue(obj_ref)

  while not candidates.empty():
    c = candidates.dequeue()
    for r in get_obj_refs_in_object(c):
      if not is_marked(r):
        mark_as_in_use(r)
        candidates.enqueue(r)
```

#### Sweep

In the sweep phase, we traverse all memory blocks in the heap (each block holds a single object/value/array) and examine each object's "in-use" flag. We then free all objects that are not in-use!

Here's some pseudocode for the sweep algorithm.

```python
# Pseudocode for the Sweep algorithm
def sweep():
    p = pointer_to_first_block_in_heap()
    end = end_of_heap()
    while p < end:
        if is_object_in_block_in_use(p):
            reset_in_use(p)      # remove the mark, object lives
        else:
            free(p)                        # free this block/object
        p = p.next
```

While we haven't seen any other approaches yet, there are some general pros and cons.

Pros:

- relatively simple
- no trouble with cyclic references (more on this later!)

Cons:
- program must be paused during GC, causing "freezes" (this is often called a "stop-the-world" GC)
  - thus, this is bad for real-time programs
- dealing with large amounts of data can lead to thrashing (at the cache/page-level)
- can cause memory fragmentation -- we'll have chunks of empty memory (more on that in a moment!)
- and, it's unpredictable!

### Mark and Compact

Mark and Compact is part of the same family of algorithms as mark and sweep. The difference is the second half of the algorithm, which is designed to prevent fragmentation. Instead of sweeping, we compact all marked/in-use objects to a new contiguous memory block. Then we can adjust the pointers to the proper relocated addresses. Our original block of memory can be treated as if it's empty, and can be reused without dealing with any sweeping.

The pros and cons are pretty similar to mark and sweep. In comparison, the only differences are:

- it much better deals with fragmentation
- but, it's more complex to implement, requires more RAM, and is slower!

### Interlude: Unpredictability

These garbage collection methods only happen when there's **memory pressure** (when the program needs memory). This means that GC is non-deterministic: you can't necessarily predict when (or even if) GC will run.

Immediately, this should make you shudder! Non-determinism is hard to reason about and affects performance. It also means you can't *rely* on the garbage collector to do things like release sockets, files, or other resources - **since you don't know when GC will run**!

In other words, you can't *really* rely on destructors! More on that in a bit :)

### Reference Counting

Reference counting takes a different approach. Instead, each object has a hidden counter that tracks how many references there are to it. Every time a reference is created or destroyed, we simply change the counter.

That sounds complicated - but, note that references can *only* change:

- with assignment (`=`)
- when an object goes out of scope

That's it! Really! If you don't believe us, think about why (or, come to office hours).

However, there are two hidden catches:

1. cyclic references are a *huge* problem; since there's a cycle, the counter will never reach zero
  - languages that use RC need to explicitly deal with cycles in a different way.
2. a "cascade" can happen: if one object is destroyed, it could remove the last reference to another object, which then gets destroyed; then, that removes the last reference to another object, ... which could cause a large chunk of deletion (and could slow your computer down)!
  - to remedy this, some languages amortize deletion over time -- but this has similar issues with non-determinism as tracing algorithms!

In summary, some pros and cons.

Pros:

- simple
- usually real-time (since reclamation is *usually* instant)
- more efficient usage since blocks are freed immediately

Cons:

- updating counts needs to be thread-safe (this is a **huge** issue!)
- updating on *every* operation could be expensive (both in time and space)
- cascading deletions
- requires explicit cycle handling

## The Ownership Model

The ownership model is another way of managing memory! 

In the ownership model, every object is "owned" by one or more variables. When the lifetime of the last variable that owns an object ends, the object is freed.

You might be thinking to yourself: this sounds super familiar! Doesn't reference counting do the exact same thing? 

And you'd be right! Both reference counting and the ownership model ulimately accomplish the same thing. For this class, we consider reference counting to be a way to implement the ownership model.

### Rust's Ownership Model: Move Semantics

You may have heard of Rust before! In the past few years, it's become one of the most beloved programming languages for its safety and rich tooling. Its implementation of the ownership model is one of the reasons the language is so safe!

In Rust, every object can only be owned by *one* variable at a time. Consider the following Rust code:

```rs
fn main() {
  let s1 = String::from("I'm owned!!");

  let s2 = s1; // Ownership transferred to s2
  println!("{}", s1); // Compiler error!
}
```

When we assign s2 to s1, we *transfer* the ownership of s1's object to s2 and *invalidate* s1. If we refer to s1 again after this transfer, we get a compiler error.

{: .note }
This is referred to as Rust's move semantics.

The same applies if we pass a variable into a function.

```rs 
fn foo(s3: String) {
  println!("{}", s3);  
} // s3's lifetime ends, string object freed

fn main() {
  let s1 = String::from("I'm owned!!");

  let s2 = s1; // Ownership transferred to s2
  foo(s2) // Ownership transferred to s3
  println!("{}", s2); // Compiler error!
}
```

Here, we've transferred the ownership of s2's object to the parameter s3. At that point s2 is invalidated and referring to it results in a compiler error.

{: .note}
You might be thinking to yourself: "Hmm... this seems super inconvenient. Why would I ever want this?" That's a valid thought! However, it's important to note that this inconvience results in much safer code and is actually a key feature of the language! It guarantees memory safety at runtime by statically requiring safe code in which it is impossible to have null pointer exceptions. In other words, the compiler enforces memory safety through the ownership model!

Rust also has an idea of "borrowing", through which we can refer to variables without transferring exclusive ownership. Here's an example:

```rs
fn foo(s3: &String) {
  println!("{}", s3);  
} // s3's lifetime ends, string object freed

fn main() {
  let s1 = String::from("I'm owned!!");

  let s2 = s1; // Ownership transferred to s2
  foo(&s2) // s2 is borrowed by foo!
  println!("{}", s2); // This is valid!
}
```

Using the & we indicate that `foo` will *borrow* `s2`, and ownership will not be transferred. After we return from `foo`, we're still able to refer to `s2`!

{: .note}
An interesting note on Rust's design philosphy: all variables are immutable by default! We must explicitly declare a variable as mutable in order to modify it. This idea carries over to borrowing. Unless marked as mutable, borrowing creates an immutable reference. This leads to much safer code! One last thing: we can only have one mutable reference to a variable at a time. Try to think about why that might be!

### C++'s Ownership Model: Smart Pointers

C++ also utilizes the ownership model[^1] through smart pointers. A smart pointer is a C++ class that works like a traditional pointer but also provides automatic memory management.

Each smart pointer object holds a traditional pointer that refers to a dynamically allocated object or array. Every smart pointer is an owner of its assigned object and is responsible for freeing it when it's no longer needed. Smart pointers can also keep track of how many references (i.e. smart pointers) there are to an object.

There are several types of smart pointers. We'll explore two here:
1. `std::unique_ptr`
2. `std::shared_ptr`

These are both fairly self explanatory based on their names: `std::unique_ptr` create exclusive references to objects while objects referred to by `std::shared_ptr` can also be referred to by other `std::shared_ptr`s.

When a `unique_ptr` goes out of scope, it frees the memory it points to automatically.

```cpp
#include<memory>  // needed for unique_ptr

#include "nerd.h"

int main() {
   std::unique_ptr<Nerd> p = std::make_unique<Nerd>("Carey", 100);
   p->study(); // p acts like a regular ptr!
}
```
It's important to note that smart_pointers disallow making copies! This includes passing it to functions.
 
The `shared_pointer` is a bit more lenient. When a `shared_pointer` goes out of scope, we coordinate amongst all the `shared_pointer`s that point at the same object and decrement the number of references to that object. In other words, `shared_pointer`s are also implemented through reference counting! Once the number of references to an object hits 0, the `shared_pointer` will automatically free the object.

```cpp
#include<memory>  // needed for shared_ptr

#include "nerd.h"

std::vector<std::shared_ptr<Nerd>> all_my_nerds;

void keep_track_of_nerd(std::shared_ptr<Nerd> n) {
  all_my_nerds.push_back(n);
}

int main() {
  std::shared_ptr<Nerd> p = std::make_shared<Nerd>("Carey", 100);
  keep_track_of_nerd(p);
}
```

Carey has great animations for each of these in his slides!


[^1]: Though it definitely doesn't feel as natural to the language as move semantics do in Rust. Understandably so as smart pointers were added in a later version of C++.

## Destructors, Finalizers and Disposers
Many objects hold resources (e.g.: file handles, network connections) which need to be released when their lifetime ends. There are three ways this is handled in modern languages, namely destructors, finalizers, and disposal methods.

### Destructors
Destructors are only used in languages with manual memory management, like C++. There are deterministic rules that govern when destructors are run, so the programmer can ensure *all* of them will run, and control *when* they run. Since the programmer can control when they run, you can use destructors to release critical resources at the right times: e.g., freeing other objects, closing network connections, deleting files, etc. 

Here is an example of destructors in action in C++.
```cpp
void doSomeProcessing() {
   TempFile *t = new TempFile();
  
   ...
 
   if (dont_need_temp_file_anymore()) 
     delete t; // explicit call to a destructor
  
   ...
}

void otherFunc() {
  NetworkConnection n("www.ucla.edu");

  ...
} // destructor for n called when it goes out of scope
```

### Finalizers
In GC languages, memory is reclaimed automatically by the garbage collector. So finalizers are used to release unmanaged resources like file handles or network connections, which aren't garbage collected. Unlike a destructor, a finalizer may not run at a predictable time or at all, since objects can be garbage collected at any time (or not at all). Since they can't be counted on to run, they're considered a last-line of defense for freeing resources, and often not used at all! We'll learn more about finalizers when we cover Object Oriented Programming.

Here are some examples of finalizers in Java and Python.
```java
// Java finalization example
public class SomeClass {
 
  // called by the garbage collector
  protected void finalize() throws Throwable
  {
    // Free unmanaged resources held by SomeObj 
    ...
  }
}
```
```python
# Python finalizer method
class SomeClass :
  ...

  # called by the garbage collector
  def __del__(self):
     # Finalization code goes here
     ...
```

### Disposal methods
A disposal method is a function that the programmer must manually call to free non-memory resources (e.g., network connections). You use disposal methods in GC languages because you can't count on a finalizer to run. Disposal provides a guaranteed way to release unmanaged resources when needed. However, we run the risk of forgetting to call the disposal method.

Here is an example of a disposer in C#.
```cs
// C# dispose example
public class FontLoader : IDisposable
{
  ...
    
  public void Dispose()
  {
     // do manual disposal here, e.g., free
     // temp files, close network sockets, etc.
  }
}

...

var f = new FontLoader(...);
... // use f to draw fonts
f.Dispose();
```

## Mutability

You might remember the concept of immutability from our discussion of functional programming: it's used to describe objects that are "read only." In other words, once an immutable object has been defined, it cannot be changed. 

Instead, we simply construct a new object based on the original, including any changes we would like to make. There are tons of benefits to immutability including eliminating bugs, speeding up garbage collection, and more! Let's take a closer look.

There are four approaches to immutability:
1. Class immutability: The programmer can designate that all  objects of a class are immutable after construction.
2. Object immutability: The programmer can designate some objects of a particular class as immutable –mutations are blocked to those objects!
3. Assignability immutability: The programmer can designate that a variable may not be re-assigned to a new value - but mutations can be made to the original referred-to object!
4. Reference immutability: The programmer can prevent a mutable object from being mutated via a reference that's marked as immutable

There are tons of benefits!
- eliminates aliasing bugs
- reduces race conditions in multithreaded code
- eliminates identity variability bugs
- elminates temporal coupling bugs
- removes side effects, making programs easier to reason about
- makes testing easier
- enables runtime optimizations
- enables easy caching
- objects are never left in an inconsistent state by definition

Pretty neat! 

{: .note}
That's it for this lecture! Lots of new stuff to wrap your head around. Have fun!
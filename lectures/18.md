---
title: 18. Control Palooza 3
week: 10
lecture_date: 2024-12-02
author: Einar Balan
originally_written: 2022-11-28
original_author: Einar Balan, Carey Nachenberg, and Ashwin Ranade
layout: lecture
parent: Lecture Notes
---

## Table of Contents
{: .no_toc }

{:toc}
- dummy item

{: .note}
This lecture note covers slides 61-90 of Control Palooza and 1-17 of Logic Programming.

## Introduction to Concurrency

Concurrency is when we decompose a program into simultaneously executing tasks.

The tasks can:
- run in parallel on multiple cores, OR run multiplexed on a single core
- operate on independent data, OR operate on shared mutable data/systems
- be launched deterministcally through program flow, OR be lanuched to due external event (button click in UI)

Concurrency can make our programs more efficient!

**Question:** If a concurrent program only runs on a single core, why would it be any faster than a program that runs serially?
**Answer:** Some tasks involve lots of waiting, not a lot of CPU. During that waiting time, a concurrent program can use the CPU for other useful tasks! For example, downloading multiple photos.

## Models for Concurrency

Two approaches for implementing concurrency:

**Multi-threading model:** a program creates multiple "threads" of execution that run concurrently (potentially in parallel).

The programmer explicitly "launches" one or more functions in their own threads, and the OS schedules their execution across available CPU cores.

```cpp
// Multi-threaded program (pseudocode)
void handle_user_purchase(User u, Item i) {
  if (bill_credit_card(u) == SUCCESS) {
    create_thread(schedule_shipping(u, i));
    create_thread(send_confirm_email(u, i));
  }
}
```

**Asyncronous Model**: A single-threaded loop manages a queue of tasks, executing them one at a time as they become ready.

When an event occurs (e.g., user clicks a button) the event results in a new function `f()` being added to the queue which eventually runs and handles the event.

```js
// Event-driven program (pseudocode)
function process_payment() { ... }

setup_event_associations() {
 button = create_new_button("Pay Now!");
 button.set_func(ON_CLICK, process_payment);
}
```

### Multi-threading Model

There are three key features of the multithreading model:
1. **Thread management**: each thread allows us to run operations concurrently
2. **Synchronization**: ensures that threads do not interfere when sharing resources
3. **Message passing**: allows different threads to safely send messages to each other

### Thread Management

Most languages provide libraries for thread management. Consider this example in C++:

```cpp
// C++ threads
#include <thread>

void f(int n) {
  for (int i=0;i<n;++i)
    cout << "Hi!";
}

int main() {
  std::thread t(f, 10);   
  ... // this keeps running concurrently
}
```

Some languages, like Go, even include threads as primitive objects!

```go
// Go's "goroutines" which are also threads
func f(n int) {
  for i := 0; i < n; i++ {
    fmt.Println("Hi!")
  }
}



func main() {
  go f(10)
  ... // this keeps running concurrently
 }
```

#### Fork Join

Fork Join is a common pattern for multithreading. It's essentially a concurrent version of divide and conquer:

1. first, we "fork" one or more tasks so they execute concurrently...
2. second, we wait for all those tasks to complete (aka "join") and then proceed.

{: .note }
If you're following at home ... this also sounds like a `map`-`fold` or `map`-`reduce`. Interesting...


**Example:** A parallel sort would be a good example of a problem suitable for fork-join.

```js
function sort_in_parallel(array, n) {
  t1 = run_background(sort(array[0:n/3]));
  t2 = run_background(sort(array[n/3:n*2/3));
  t3 = run_background(sort(array[n*2/3:n]));

  wait_for_all_tasks_to_finish(t1, t2, t3);
  merge_sorted_subarrays(array);
}
```

```js
function task1(param) { ... }
function task2(param) { ... }
function task3(param1, param2) { ... }

function launch_in_parallel() {
  t1 = run_background(task1(42));
  t2 = run_background(task2("suss"));
  t3 = run_background(task3(3.14, 2.71));

  wait_for_all_tasks_to_finish(t1, t2, t3);
  print("All tasks completed, proceeding...");
}
```

Fork-join is often used recursively! For huge datasets, we may choose to split a data source in two (forking). But, if it's still too big, we can split it again! You can imagine this repeating until each chunk is small enough to deal with independently.

#### Fork-Join In Different Languages

```cpp
// C++
#include <thread>

void task1(int n)  { ... }
void task2(int n)  { ... }
void task3(int n)  { ... }

int main() {
  std::cout << "Forking threads!\n";
  //To launch a function in the background, we simply create a new thread object.

  std::thread t1(task1, 10);
  std::thread t2(task2, 20);
  std::thread t3(task3, 30);

  // do other processing here...

  t1.join(); //To join, we simple call the thread.join() method. This blocks the caller until the thread finishes.

  t2.join();
  t3.join();
  std::cout << "All threads joined!\n";
}
```

```cs
// C#
using System;
using System.Threading;
using System.Threading.Tasks;

class Program {
  static int task1(int param) { ... }
  static int task2(int param) { ... }
  static int task3(int param) { ... }

  public static void Main(string[] args) {
    int r1 = 0, r2 = 0, r3 = 0;
    Console.WriteLine("Forking threads!");
    var t1 = Task.Run(() => r1 = task1(10));
    var t2 = Task.Run(() => r2 = task2(20));
    var t3 = Task.Run(() => r3 = task3(30));
    Task.WaitAll(t1, t2, t3);
    Console.WriteLine("Joined: {0}, {1}, {2}",
                      r1, r2, r3);
  }
}
```


```py
# Python
import threading

def task(n):
  while n > 0:   # do some computation
    n = n - 1

print("Forking threads!")

#In Python, we must create a thread object, and then do thread.start() before it runs.

t1 = threading.Thread(target=task, args=(100000000,))
t2 = threading.Thread(target=task, args=(100000000,))
t3 = threading.Thread(target=task, args=(100000000,))
t1.start()
t2.start()
t3.start()

# do other processing here...

t1.join()
t2.join()
t3.join()
print("All threads joined!")
```

#### "Multi-threading" in Python

**Question for the Python program:**
Assuming looping 100 million times takes 5s, how long will this program take to run on a multicore PC?

**Answer**: 15s
We'd expect all three tasks to run in parallel... taking 5s total. But it takes 15s!
Why? Because when each Python thread runs, it claims exclusive access to Python's memory/objects.

So only one thread generally does computation at a time!

Why? Python's garbage collection system was never designed to be thread-safe!

Python has something called a "Global Interpreter Lock" or GIL. The GIL is like a hot potato – it can only have one owner at a time. Once a thread takes ownership of the GIL it's allowed to read/write to Python objects. After a thread runs for a while, it releases the GIL (tosses the potato) to another thread and gives it ownership. If a thread is waiting for the GIL, it simply falls asleep until it gets its turn.

**Question:** So why even support multiple threads? Are there any cases where multithreading speeds things up?

**Answer:** Answer: I/O operations like downloading data from the web or saving a file to disk DON'T need the GIL to run! So these kinds of operations can still make progress if launched in the background!

### Concurrency With Shared Mutable State

**Question:** What will the value of karma be if I run both functions concurrently until completion? Why?

```cpp
int karma = 0;

void do_good_deeds() {        // runs in thread t1!
  for (int i=0;i<100000;++i)
    karma++;
}

void be_naughty() {           // runs in thread t2!
  for (int i=0;i<100000;++i)
    karma--;
}
```

**Answer:** Given that each "thread" of execution can be interrupted at any time to run the other, it's impossible to know!

The undefined behavior is due to both threads using a _shared mutable state_; `karma` is a mutable variable that's shared between multiple threads. To take a closer look, let's consider the most basic operations that make up `karma++`.

1. Reading the `karma` variable from memory
2. Incrementing the value that is read
3. Storing the new incremented value back into `karma`'s memory address

If we have two threads accessing `karma`, these basic operations can be interleaved differently, resulting in unpredictable, nondeterministic behavior. In short: this is not good! Consider the following potential sequences accross two threads:

```
do_good_deeds: read karma (value: 0)
do_good_deeds: increment 0, the value that was read (value: 1)
be_naughty:    read karma (value: 0)
do_good_deeds: write 1 to karma
be_naughty:    decrement 0, the value that was read (value: -1)
be_naughty:    write -1 to karma
```

Here the final value stored in `karma` is -1.

```
do_good_deeds: read karma (value: 0)
do_good_deeds: increment 0 (value: 1)
be_naughty:    read karma (value: 0)
be_naughty:    decrement 0 (value: -1)
be_naughty:    write -1 to karma
do_good_deeds: write 1 to karma
```

Here the final value stored in `karma` is 1. Each thread reads in the intitial value of `karma` and performs some operation on it. The unpredictability comes from the fact that the order in which we write the resulting value back to `karma` in each thread might vary. This is called a race condition and clearly it's a big problem! How might we avoid it?


### Safe Concurrency With Shared Mutable State

Most modern languages now have built-in language features to make it safer to use shared mutable state. To list a few: mutexes, semaphores, spinlocks, barriers, conditions, and read write locks. Lots of funny words but they are very useful!

Here are a few examples:

```cpp
// C++ example of using mutex objects
#include <thread>
#include <mutex>

int counter = 0;
std::mutex mtx;

void increment(int times) {
  for (int i = 0; i < times; ++i) {
    mtx.lock();      // Lock mutex
    counter++;       // Critical section
    mtx.unlock();    // Unlock mutex  
  }
}

int main() {
  // Create two threads running increment(1000)
  std::thread thread1(increment, 1000);
  std::thread thread2(increment, 1000);
  ...
}
```
In C++, we "lock" mutexes to indicate a section of our code that only one thread can access at a given time. This is useful for managing shared mutable state across threads, such as a counter variable!

Here's another example in Ruby:

```ruby
// Ruby example of using mutex objects
require 'thread'

counter = 0
mutex = Mutex.new

increment = ->(times) {
  times.times do
    mutex.synchronize { 
      counter += 1 
    }
  end
}

# Create two threads running increment(1000)
thread1 = Thread.new { increment.call(1000) }
thread2 = Thread.new { increment.call(1000) }
...
```

Some languages have built in synchronization primitives! Consider Java's `synchronized` keyword:

```java
// Java has a built-in keyword for synchronization
public class Counter {
  private Object syncObj = new Object();
  private int counter = 0;

  public void increment(int times) {
    for (int i = 0; i < times; i++) {
      synchronized(syncObj) {   
        counter++;
      }
    }
  }

  public static void main(String[] args) throws Exception {
    Counter counter = new Counter();

    Thread thread1 = new Thread(() -> counter.increment(1000));
    Thread thread2 = new Thread(() -> counter.increment(1000));
    ...
  }
}
```

Here the `synchronized` keyword requests exclusive ownership of `syncObj`. If another thread reaches this point while the original thread still has ownership, it will be blocked until ownership is released. 

### Message Passing

To reduce the use of shared mutable state and race conditions, some languages use built-in message queues to enable threads to safetly communicate.

Consider this example in Go:

```go
// Go "channels" used for message passing
func genPrimes(messages chan int) {
  for i := 2; ; i++ { // Infinite loop  
    if isPrime(i) {
      messages <- i // Send message
    }
  }
}

func main() {
  messages := make(chan int)

  go genPrimes(messages) // launch thread

  for {
      prime := <-messages // dequeue next prime
      fmt.Println(prime)
  }
}
```

Here's another example in Erlang:

```erlang
// Erlang message passing example
gen_primes(pid, N) ->
  if is_prime(N) -> pid ! {prime, N}; 
     true        -> ok
  end,
  gen_primes(LoopPid, N + 1).

loop() ->
  receive
    {prime, Prime} ->
      io:format("~p~n", [Prime]),
      loop()
  end

start() ->
  pid = self(),
  spawn(fun() -> gen_primes(pid, 2) end),
  loop().
```

### Asyncronous Programming

Let's spend a bit of time to see the other approach -- asyncronous programming!

In asyncronous programming, statements in a program are not executed from top to bottom. Instead, a module called a **runtime** maintains a queue of **coroutines** to execute. A coroutine is a function that once run, can be paused and resumed. The runtime repeatedly dequeues, suspends, and resumes coroutines resulting in a concurrent execution of each of them. I/O operations can even be performed outside the queue while a coroutine is running.

Async programming becomes very useful when we have an IO bound task, such as reading from a databse or downloading web data. While waiting for an IO operation to be completed, the runtime can suspend a coroutine and enqueue other coroutines. Coroutines are also very lightweight in terms of memory usage compared to threads! 

Carey's slides have a great animated example that we can't really do justice here. Go check it out!

Here's an example in Python:

```python
from asyncio import run, create_task


async def g(x):
  return x + "!"


async def f(x):
  y = await g(x)
  return "Hello " + y


async def main():
  task1 = create_task(f("world"))
  task2 = create_task(g("async"))
  r1 = await task1
  r2 = await task2
  print(f"Results: {r1} {r2}")
```

Here's a more practical example in which a database query is performed on a web server:

```python
from aiohttp import web
import asyncpg

async def handle_request(request):
  product_name = request.query.get('product_name')
  query = "SELECT product, price FROM products WHERE product_name = $1"


  connection = await asyncpg.connect(dsn='postgres://user:password@localhost/mydatabase')
  result = await connection.fetch(query, product_name)
  await connection.close()

  if result:
    response_text = f"{result['product']}: ${result['price']}"
  else:
    response_text = "No matching product found."

  return web.Response(text=response_text)

app = web.Application()
app.add_routes([web.get('/prod_search', handle_request)])

web.run_app(app)
```

### Multithreading vs. Async

Why use one model over another? Let's see!

| Multithreading | Asynchronous Programming |
|----------------|--------------------------|
| Capable of true parallelism across cores | Concurrency without true parallelism |
| Preemptive multitasking | Cooperative mulitasking |
| Uses more memory; each thread has extensive execution context | Uses less memory; each coroutine is lightweight |
| Race conditions are common, requiring synchronization | Race conditions are less common due to single threaded nature |
| Best suited for CPU bound tasks | Best suited for IO bound tasks |
| Complex error handling | Easy error handling |

In summary: concurrency is a super powerful tool in programming! We explored a few different ways it is implemented and the implications of each.


##  Prolog and Logic Programming

**Logic programming** is a paradigm where we express programs as a set of facts (relationships which are held to be true), and a set of rules (i.e. if A is true, then B is true).

A program/user then issues queries against this these facts and axioms:

Logic programming is declarative – programs specify "what" they want to compute, not "how" to do so.

Examples:

**Facts:**
- Martha is Andrea's parent
- Andrea is Carey's parent

**Rules:**
If X is the parent of Q, and Q is the parent of Y, then X is the grandparent of Y

**Queries:**
- Is Martha the grandparent of Carey?
- Who is the grandparent of Carey?

### Introduction to Prolog

Prolog is used for theorem-proving (type checking is one such use case; it's in the Java compiler!) and has also been used to implement natural language processing and expert systems.

Prolog mostly leans on *declarative* programming: you specify what you want, but now *how* you should get there; Prolog is responsible for figuring that out!

When given a query, Prolog tries to find a chain of connections between the query and the specified facts and rules that lead to an answer.

Every Prolog program is comprised of facts about the world, and a bunch of rules that can be used to discover new facts.

### Prolog Facts

A "fact" is a predicate expression that declares either...
- An attribute about some thing (aka an atom); you can think of this as being "always true"
  - `outgoing(ren)`
- A relationship between two or more atoms or numbers:
  - `parent(alice, bob)`
  - `age(ren, 80)`
  - `teaches(carey, course(cs, 131))`

Implicitly, we're defining some terms:

- an "atom" is a "thing". In the above examples, this includes `ren`, `alice`, and `bob`. They **must be lowercase**.
- a "functor" is like a function (or relationship). In the above examples, this includes `outgoing`, `parent`, and `age`. They **must be lowercase**.

If a fact isn't explicitly stated, it is false. For example, in the above case, since we didn't explicitly state it, `alice` is not outgoing.

### Prolog Rules

A rule lets us define a new fact in terms of one or more existing facts or rules.

Each rule has two parts...
- A "head" which defines what the rule is trying to determine
- A "body" which specifies the conditions (aka subgoals) that allow us to conclude the head is true

Rules can be defined with atoms, numbers, or *variables*. Variables like `X` and `Y` are like placeholders which Prolog will try to fill in as it tries to answer user queries. **Variables must always be capitalized**.


```prolog
% Facts:
outgoing(ren).
silly(ren).
parent(alice, bob).
age(ren, 80).
parent(bob, carol).

% Rules:
comedian(P) :- silly(P), outgoing(P).
grandparent(X, Y) :- parent(X,Q), parent(Q,Y).
old_comedian(C) :- comedian(C),
age(C, A), A > 60.
```

{: .note }

Rules and facts might seem very different, but some would argue that they aren't! You could think of a fact as a rule that has no body. Then, it is [vacuously true](https://en.wikipedia.org/wiki/Vacuous_truth) (and thus, always true).

#### Recursive Rules

Rules can also be recursive and can have multiple parts!

Similar to recursion and pattern matching in Haskell, Prolog processes rules from the top to the bottom. So, the base case(s) should always go first!

```prolog
% Recursive rules:
ancestor(X, Z) :- parent(X,Z).
ancestor(X, Z) :- parent(X,Y), ancestor(Y,Z).
```

Explanation of the above code block:

X is the ancestor of Z if:
- X is Z's parent
- OR if X is some person Y's parent AND Y is an ancestor of person Z

{: .note }
Prolog uses something called the "closed-world assumption" (CWA). Basically, this means that only things that can be proven true by the program are true; *everything else* is false.

#### Negation in Rules

Rules can also use negation – but be careful!

In Prolog, `not(something)` works as follows:

1. Prolog tries to prove `something` is true using all of the program's facts and rules
2. If `something` can't be proven as true, then `not(something)` is found to be true.


```prolog
% Rules with negation:
serious(X) :- not(silly(X)).
```

For example, the query `serious(alice)` is true, since the closed-world assumption says that `silly(alice)` is false.

## Prolog Queries
We can create queries to answer true/false questions:
- A query can match a simple fact...
- Or it can execute a rule.

Given this set of facts and rules (also known as a knowledge base):
```
% Facts:
outgoing(alice).
parent(alice, bob).			 
parent(alice, brenda).
parent(brenda, caitlin).	
parent(caitlin, ned). 		 

% Rules:
grandparent(X, Y) :- parent(X,Q), parent(Q, Y). 
```

We can execute these queries. Prolog will then search through the knowledge base to find the answers.

```prolog
?- outgoing(alice)
true
?- grandparent(brenda, ned)
true
```

The first query essentially asks, "Is Alice outgoing?" One of our facts (the first in our knowledge base) says she is, so the result is `true`. The second query asks, "Is Brenda the grandparent of Ned?" Based on our `grandparent` rule, Prolog determines that she is and the result is true.

This is cool, but Prolog can do more than just answer yes or no questions! We can also retrieve certain information, like "who are Alice's parents?"
- The query will find ALL possible matches.
- The query can specify multiple unknowns.
- And Prolog will find ALL consistent combinations of answers!

```prolog
?- parent(alice, Who)
Who = bob
Who = brenda


?- grandparent(A, B)
A = alice, B = caitlin
A = brenda, B = ned
```

The variables `Who`, `A`, and `B` indicate an unknown that we want Prolog to fill in. Our first query is essentially asking "Who are Alice's parents?" Based on our knowledge base, Prolog deteremines that Bob and Brenda are her parents. Our second query asks to find all grandparents and their grandchildren.

{: .note}
A quick reminder on terminology: `alice` is an atom, `Who` is a variable, and `parent(alice, Who) is a query.



## Resolution: How Prolog Answers Queries

Example facts/rules:

```prolog
parent (nel, ted).
parent (ann, bob).
parent (ann, bri).
parent (bri, cas).
gparent( X ,  Y )  :-
  parent(X,Q), parent(Q,Y).
```

Example query:

```prolog
gparent(ann, W)
```

Algorithm:
1. Add our query to a goal stack
2. Pop the top goal in our goal stack and match it with each item in our database, from top to bottom
3. If we find a match, extract the variable mappings, and create a new map with the old+new mappings
4. Create a new goal stack, copying over existing, unprocessed goals and adding new subgoals
5. Recursively repeat the process on the new goal stack and the new mappings

So initially, our goal stack is simply `gparent(ann,W)`, and our mappings are `{}`.
Then, after matching the last rule, our goal stack becomes:


```
parent(X,Q)
parent(Q,Y)
```
And our mappings are `{X->ann,W<->Y}`

Then, pop the top rule off our goal stack. We have a potential match with `parent (nel, ted)`. So our goal stack becomes:

```
parent(Q,Y)
```
With the mappings: `{X->ann,W<->Y, Q->bob}`

We pop the top rule off the stack again. There's no rule that matches `parent(bob,Y)`, which means the set of mappings we discovered were not valid.

So we'll back-track one level up and continue searching for alternative mappings.

If our goal stack is empty, we output variables from our mapping that were explicitly queried; in this case, our final mapping is:
`{ X ->ann, W <-> Y <-> cas, Q ->bri }`

So we return `cas`, since `W=cas`.
